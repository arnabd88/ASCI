\documentclass{article}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\newcommand\algo{\vspace{.10in}\textbf{Algorithm: }}
\newcommand\correctness{\vspace{.10in}\textbf{Correctness: }}
\newcommand\runtime{\vspace{.10in}\textbf{Running time: }}
\newcommand\pseudoCode{\vspace{.10in}\textbf{PseudoCode: }}
\newcommand*{\perm}[2]{{}^{#1}\!P_{#2}}
\newcommand*{\comb}[2]{{}^{#1}\!C_{#2}}
%\pagestyle{fancyplain}
%\lhead{\textbf{\NAME\ (\UID)}}
%\chead{\textbf{Hw\HWNUM}}
%\rhead{CS 6150, \today}
\title{CS6210 - Homework/Assignment-3}
\author{Arnab Das(u1014840)}
\usepackage[utf8]{inputenc}
\begin{document}
  \pagenumbering{gobble}
  \maketitle
  \newpage
  \pagenumbering{arabic}
  \newcommand\NAME{ArNaB DaS}
  \newcommand\UID{uxxxxxxx}
  \newcommand\HWNUM{3}
  
   \question{Question-1}{Chapter-4: Exercise-12}
     The condition number of an eigen value, $\lambda$ , of a matrix A is defined as \newline
     \[ s(\lambda) = \dfrac{1}{\textbf {x}^T\textbf{w}}\] \newline
     Referencing from example 4.7/4.6, let is define the two matrices as: \newline
     $A_{1}$ = 
$\begin{bmatrix}
  4 & 0 \\
  0 &  4 \\
\end{bmatrix}$ and $A_{2}$ = 
$\begin{bmatrix}
 4 & 1 \\
 0 & 4 \\
\end{bmatrix}$ \newline

Both the matrices have eigen value of 4 with algebraic multiplicaity 2, that is both its eigen values are 4,4. \newline
Now first let us consider $A_{1}$. Its eigen vectors corresponding to eigen valuesm of 4 are \textbf {$x_{1}$} = $\begin{bmatrix} 1 \\ 0 \\ \end{bmatrix}$ and $x_{2}$ = $\begin{bmatrix} 0 \\ 1 \\ \end{bmatrix}$. Thus the geometric multiplicity is also 2. The left eigen vectors, \textbf {$w^T$}, will be $w_{1}^T$ = $\begin{bmatrix} 1 & 0 \\ \end{bmatrix}$ and $w_{2}^T$ = $\begin{bmatrix} 0 & 1 \\ \end{bmatrix}$. \newline
Thus, for each of the above eigen vectors for $A_{1}$, the inner product $\dfrac{1}{\textbf {x}^T\textbf{w}}$ is 1, hence the condition number turns out to be, \newline
\begin{equation}
  S(\lambda = 4)_{A_{1}} = 1
\end{equation}
Let us consider $A_{2}$ for now. The eigen values for $A_{2}$ is 4 with algebraic multiplicity 2. However , it has only one right eigen vector, $x_{1}$ = $\begin{bmatrix} 0 \\ 1 \\ \end{bmatrix}$, and only one left eigen vector, $w_{1}^T$ = $\begin{bmatrix} 0 & 1 \\ \end{bmatrix}$. \newline
Thus, for the above pair of left and right eigen vector for $A_{2}$, the inner product $\dfrac{1}{\textbf {x}^T\textbf{w}}$ is $\dfrac{1}{0} = \infty$. Hence the condition number turns out to be \newline
\begin{equation}
  S(\lambda = 4)_{A_{2}} = \infty
\end{equation}
Condition numbers indicate how stable the computation is expected to be, such that lower computation numbers indicate more stability. If we refer to example-4.7, where the experiment was done with small perturbation to the matrix, $A_{1}$ came to be well-conditioned while $A_{2}$ came to be ill-conditioned. Our evaluation of the condition number also suggests that since condition number for $A_{1}$ is small it is numerically more stable and hence well conditioned while $A_{2}$ has condition number number of $\infty$ and hence ill-conditioned. \newline

\question{Question-2}{Chapter-5: Exercise-2}
For an $n \times n$ matrix A, and a vector vector b, the pseudoCode for the Gauss-Jordan elimination method for Solving $Ax = b$, is as described below(Assuming no pivoting):

\part{a}
\pseudoCode \newline
\hspace*{0.5cm} \textbf {for} k=1 : $n-1$ \newline
\hspace*{1cm}      \textbf {for} i=1 : n \newline
\hspace*{1.5cm}     \textbf {if} (i $\neq$ k) \newline
\hspace*{2cm}            $l_{i,k}$ = $\dfrac{a_{i,k}}{a_{k,k}}$ \newline
\hspace*{2cm}               \textbf {for} $j=k+1 : n$ \newline
\hspace*{2.5cm}               $ a_{i,j}$ =  $a_{i,j} - l_{i,k}a_{k,j}$ \newline
\hspace*{2cm}              $b_{i}$ = $b_{i} - l_{i,k}b_{k}$
  
  Since, it does the update for all rows except the row k, one \textbf{ if} condition is introduced to check for $i \neq k$, and the row traversal instead of $k+1$ to n, has been increased as 1 to n.



\part{b} The cost of the Gauss-Jordan algorithm in terms of operation count(or flop count) is as follows: \newline
\begin{align*}
  \sum_{k=1}^{n-1} 2(n-1)(n-k) + 2(n-1) + (n-1) \\
 &=\sum_{k=1}^{n-1} 2(n-1)(n-k+1) + (n-1) \\
&=(n-1) \sum_{k=1}^{n-1} 2(n-k+1) + 1 \\
&=(n-1) \sum_{k=1}^{n-1}(2n - 2k + 3) \\
&=(n-1) \bigg( 2n(n-1) -2\dfrac{n(n-1)}{2} + 3(n-1) \bigg) \\
&=(n-1)^2 \bigg( 2n - n + 3 \bigg) \\
&=(n-1)^2 ( n+3 ) \\
&=n^3 - 2n^2 + n + 3n^2 -6n + 3 \\
&= n^3 + O(n^2) \\
\end{align*}
(Proved).


\question{Question-3}{Chapter-5: Exercise-3}
Let A and T be two non-singular, $n \times n$ matrices. Furthermore, we are given two matrices, L and U such that L is unit lower triangular and U is upper triangular and the following relation holds: \newline
\begin{equation}
   TA = LU
\end{equation}
The algorithm to find the solution for $Ax=b$ is detailed below

\algo:\newline
\textbf {Step-1:} Perform a matvec operation to evaluate \textbf {Tb}. \newline
\textbf {Step-2:} Solve for y: Ly = Tb ; //By forward substitution \newline
\textbf {Step-3:} Solve for x: Ux = y ; //By backward substituion \newline

  \textbf {Explanation: } To evaluate Ax=b, from the given conditions, we first perform a matvec of T and b which is $O(n^2)$, since it invloves a matrix-vector multiplication of $n \times n$ matrix T, and a $n \times 1$ vector b. In second step we solve for y using forward substitution since there is a lower triangular matrix. This step also involves $O(n^2)$ operations. The third step involves the evaluation of the final solution \textbf {x}, and since there is an upper triangular matrix, we use backward substitution which is again $O(n^2)$. Thus total flops required is in order of $O(n^2)$. \newline

\pseudoCode \newline
\hspace*{0.5cm} \textbf {SolveForX} (L, U, T, b, x) \newline
\hspace*{1cm}       \textbf {for} i=1:n \newline
 \hspace*{1.5cm}       sum = 0 \newline
 \hspace*{1.5cm}       for j=1:n \newline
  \hspace*{2cm}              sum +=$ T_{i,j}$ *$ b_{j}$ \newline
 \hspace*{1.5cm}      $G_{i}$ = sum \newline
 \hspace*{1cm}    // G is the matvec result of Tb \newline
 \hspace*{1cm}   // Solve for Ly = G = Tb by forward substitution \newline
 \hspace*{1cm}   \textbf {for} k=1:n \newline
 \hspace*{1.5cm}    $y_{k}$ = $\dfrac{G_{k} - \sum_{j=1}^{k-1}L_{k,j}y_{j}}{L_{k,k}}$ \newline
\hspace*{1cm}  // Solve for Ux = y by backward substitution \newline
\hspace*{1cm}   \textbf {for} k=1:n \newline
\hspace*{1.5cm}   $x_{k}$ = $\dfrac{y_{k} - \sum_{j=k+1}^n U_{k,j}x_{j}}{U_{k,k}}$

  
  \question{Question-4}{Chapter-5: Exercise-4}
To find the inverse of a matrix, A , one of the simplest mechanism to do so in linear algebra is to augment the matrix A with a identity matrix of the same size. Then perform a sequence of operations such that the region of A is replaced by an identity matrix and in that case the region of the augmented matrix where the identity matrix was initiallly added, contains the inverse of the matrix, A. \newline
Suppose , we are given an $n \times n$ matrix whose matrix we need to find. Suppose by performing k number of matrix operations, each operation represented as $T_{1}$, we can derive the identity matrix from A, which means we can write the following: \newline
$T_{1}T_{2}T_{3} \dots T_{k} A = I$ \newline
Multiply both sides by the inverse: \newline
$T_{1}T_{2}T_{3} \dots T_{k} A A^{-1}= IA^{-1}$ \newline
$T_{1}T_{2}T_{3} \dots T_{k} I= A^{-1}$ \newline
which means the same sequence of operations, when applied on the diagonal matrix of the same size, will yeld the inverse of the matrix,A. \newline

Once, we augment A with  the diagonal matrix, the size of the overall matrix becomes $n \times 2n$. Let us call this matrix B. The pseudocde for obtaining inverse from B using GaussJordan is described below: \newline

\pseudoCode \newline
\hspace*{0.5cm} \textbf {for} k=1 : $n-1$ \newline
\hspace*{1cm}      \textbf {for} i=1 : n \newline
\hspace*{1.5cm}     \textbf {if} (i $\neq$ k) \newline
\hspace*{2cm}            $l_{i,k}$ = $\dfrac{b_{i,k}}{b_{k,k}}$ \newline
\hspace*{2cm}               \textbf {for} $j=k+1 : 2n$ \newline
\hspace*{2.5cm}               $ b_{i,j}$ =  $b_{i,j} - l_{i,k}b_{k,j}$ \newline
\hspace*{1cm}    diag\_temp = $b_{k,k}$ \newline
\hspace*{1cm}    \textbf for $j=k:2n$ \newline
\hspace*{1.5cm}                 $b_{k,j} = \dfrac{b_{k,j}}{diag\_temp}$ \newline
%\hspace*{2cm}              $b_{i}$ = $b_{i} - l_{i,k}b_{k}$

The computation cost required for thisn operation in terms of floating point operations can be computed as follows: \newline
  
\begin{align*}
  \sum_{k=1}^{n-1} 2(2n-k)(n-1) + (n-1) + (2n - k + 1) \\
 &= \sum_{k=1}^{n-1} (4n - 2k)(n-1) + 3n - k \\
 &= \sum_{k=1}^{n-1} 4n^2 -4n -2nk + 2k + 3n - k \\
 &= \sum_{k=1}^{n-1} 4n^2 -n -2nk + k \\
&= 4n^2(n-1) -n(n-1) -n^2(n-1) +\dfrac{n(n-1)}{2} \\
&= 3n^3 + O(n^2) \\
\end{align*}

The same task performed using LU Decomposition requires $\dfrac{8}{3}n^3 + O(n^2)$ operation count. Thus, in the order of $n^3$, this method requires $\dfrac{1}{3}n^3$ more operation count. 


\question{Question-5}{Chapter-5: Exercise-12}
 To reformat the cholesky algorithm from the naive one given with multiple for-loops, below is the pseducode for the vectorized one: \newline

\pseudoCode \newline
\hspace*{0.5cm} \textbf {for} k=1:n-1 \newline
\hspace*{1cm}                       $a_{k,k} = \sqrt{a_{k,k}}$ \newline
\hspace*{1cm}                      $A_{k+1:n, k}$ = $\bigg (  \dfrac{a_{k+1,k}}{a_{k,k}} , \dfrac{a_{k+2,k}}{a_{k,k}} \dots ,\dfrac{a_{n,k}}{a_{k,k}}\bigg )$ \newline
\hspace*{1cm}                      $A_{k+1:n,k+1:n}$ = $A_{k+1:n,k+1:n}$ - $[A_{k+1:n,k+1:n}]$ * $[A_{k+1:n,k+1:n}]^T$ \newline
\hspace*{1cm}                     $A_{k,k+1:n}$ = 0 \newline
\hspace*{0.5cm}   $A_{n,n} = \sqrt(A_{n,n})$ \newline

The implemented code in Matklab is available as $Prob5\_Cholesky.m$  in the associated submission tar. Run in the matlab window as :\newline
$Prob5\_Cholesky(n)$;  // where n is the size of the matrix. It generates an $n \times n$ matrix and reports the result of the implemented cholesky and the matlab cholesky function to compare the output. \newline


\question{Question-6}{Chapter-5: Exercise-20}
\textbf {Hessenberg Matrix:} An $n \times n$ matrix A is said to be in Hessenberg or upper Hessenberg form if all its elements below the first subdiagonal are zero, such that:\newline
\[ a_{i,j} = 0, i>j+1\]

\part{a}
The psedoCode for LU decomposition of an upper Hessenberg Matrix,A of size $n \times n$ is as follows:\newline
\pseudoCode \newline
\hspace*{0.5cm} \textbf {for} k = 1:n-1 \newline
\hspace*{1cm}      $l_{k+1,k}$ = $\dfrac{a_{k+1,k}}{a_{k,k}}$ \newline
\hspace*{1cm}    \textbf {for} j=k+1:n \newline
\hspace*{1.5cm}     $ a_{k+1,j}$ = $a_{k+1,j} - l_{k+1,k}*a_{k+1,j}$ \newline

The upper traingular part of A will contain the U matrix formed due to the decomposition. We have not updated the zero values here. 

\part{b}
The sparsity structure of the matrix L will look as follows: \newline
L = $\begin{bmatrix} 
1 & 0 & 0 & 0 & 0 &\dots & 0 \\
l_{2,1} & 1 & 0 & 0 & 0 &\dots & 0 \\
0 & l_{3,1} & 1 & 0 & 0 & \dots & 0 \\
0 & 0 & l_{4,1} & 1 & 0 & \dots & 0 \\
\dots & \dots & \dots & \dots & \dots & \dots \\
\dots & \dots & \dots & \dots & \dots & \dots \\
0 & 0 & 0 & 0 & \dots &    l_{n,n-1} & 1 \\

\end{bmatrix}$

Thus, L contains one subdiagonal below the main diagonal. \newline

\part{c} 
The cost of operation for the decomposition  can be evaluated as: \newline
\begin{align*}
   \sum_{k=1}^{n-1} 2(n-k) + 1 \\
   &= 2n(n-1) -n(n-1) + (n-1) \\
   &= O(n^2) \\
\end{align*}
   
To solve Ax = b or LU = b, where A=LU, we solve Ly=b first using forward substituion and then Ux=y using backward substituion. Since L is banded, the general formula for forward substituion in this case will become: \newline
\textbf {for} k=1:n-1 \newline
\hspace*{0.5cm} $y_{k}$ = $\dfrac{b_{k} - L_{k,k-1}*y_{k-1}}{L_{k,k}}$ \newline
The cost for forward substituion will be 3n $\approx$ O(n)

For the backward substituion for Ux=y, will still be $O(n^2)$ since the upper traingle remains dense and does not have a sparsity pattern. \newline
Therefor, \textbf {total cost of operation} = $O(n^2) + O(n) + O(n^2) \approx O(n^2)$ \newline


\part{d}
If we enable partial pivoting for the LU decomposition of A, then we get \textbf {PA = $L^\prime U^\prime$}, where $L^\prime$ is the corresponding lower triangular matrix and $U^\prime$ is the corresponding upper matrix for PA. The sparcity structure of $L^\prime$ will be same as L. However, since we have applied partial pivoting, we need to recober the factors of A. The permutation matrix has the following property ,\newline
\[PP^T = I\]
\[=> P^T = P^{-1}\]

Thus the folowing holds if we multiply \textbf {PA = $L^\prime U^\prime$} by $P^T$: \newline
\begin{align*}
P^TPA =& P^TL^\prime U^\prime \\
A =& (P^TL^\prime )U^\prime \\
\end{align*}

Since L and $L^\prime$ has the same sparsity structure, the product $P^TL^\prime$ will destroy the sparsity structure of $L^\prime$, however the number of zeros for every column will still remain the same. \newline



\question{Question-7}{Chapter-5: Exercise-21}
Given arrow matrices: \newline

A = $\begin{bmatrix} 
x & x & x & x & \dots & x \\
x & x & 0 & 0 & \dots & 0 \\
x & 0 & x & 0 & \dots& 0 \\
\dots & \dots & \dots & \dots & \dots & \dots \\
x & 0 & 0 & \dots & x & 0 \\
x & 0 & 0 & \dots & 0 & x \\
\end{bmatrix}$ and 
B = $\begin{bmatrix}
x & 0 & 0 & \dots & 0 & x \\
0 & x & 0 & \dots & 0 & x \\
0 & 0 & x & \dots & 0 & x \\
\dots & \dots & \dots & \dots & \dots & \dots \\
0 & 0 & 0 & \dots & x & x \\
x & x & x & \dots & x & x \\
\end{bmatrix}$ where $x$ denotes non-zero values. \newline 
For the matrix, A, even though it is sparse arrow matrix, even in the first step, as we try solve for Gauss-elimination, a lot of fill-in occurs while trying to zero out the first column. Thus it becomes dense after the first step. This means, rest of the computation for matrix A will have the computation cost same as the dense matrix computations solved earlier , that is $O(n^3)$ while the storage will require full matrix storage, that is , $O(n^2)$. \newline

Interesting things happen with the B matrix since its sparsity structure can be effectively utilized while zeroing out the column elements. The following modified pseudocode will suit the operations on the matrix B for generating the L and U matrices. \newline

\pseudoCode \newline
\hspace*{0.5cm} \textbf {for} k=1:n-1 \newline
\hspace*{1cm}        $l_{n,k}$ = $\dfrac{a_{n,k}}{a_{k,k}}$ \newline
\hspace*{1cm}        $a_{n,k}$ = 0 \newline
\hspace*{1cm}        $a_{n,n}$ = $a_{n,n} - l_{n,k}*a_{k,n}$ \newline
\hspace*{1cm}        $b_{n}$  = $b_{n} - l_{n,k}*b_{k}$ \newline

At a specific k, we do not iterate over the rest of the rows after k, except the last row, since others will have their values in k'th column as 0. Also, when we are considering the last row, effectively we are zeroing out its element in the k'th column, hence assigning $a_{n,k}=0$. Since in the k'th row, we have non-zeros only in the k'th column and the last column, hence the update rule applies only to the n;th column in the n'th row. Similarly, we can also update the b vector's $b_{n}$ element. \newline

Thus the cost of operation becomes = \textbf {5(n-1)} $\approx O(n)$. \newline

In general the L and U matrices can be stored together in a compact form as shown below, with the original matrix discarded\newline

$\begin{bmatrix}
 u_{11} & u_{12} & u_{13} & \dots & u_{1n} \\
  l_{21} & u_{22} & u_{23} & \dots & u_{2n} \\
  l_{31} & l_{32} & u_{33} & \dots & u_{3n} \\
 \dots & \dots & \dots & \dots & \dots \\
  l_{n1} & l_{n2} & \dots & l_{n,n-1} & u_{nn} \\
\end{bmatrix}$, where we are not storing the unit diagonal elements of the L matrix. \newline

Similarly, we can store the $l_{i,j}$ and $u_{i,j}$ values in such a compact way , such that the matrix looks as below: \newline
C = $\begin{bmatrix}
 u_{11} & 0 & 0 & \dots & u_{1n} \\
  0 & u_{22} & 0 & \dots & u_{2n} \\
  0 & 0 & u_{33} & \dots & u_{3n} \\
 \dots & \dots & \dots & \dots & \dots \\
  0 & 0 & \dots & u_{n-1,n-1} & u_{n-1,n} \\
  l_{n1} & l_{n2} & \dots & l_{n,n-1} & u_{nn} \\
\end{bmatrix}$

Thus storage required in this case is $\approx 3n$ or $\approx O(n)$ .

For solving the LUx=b, we first solve Ly=b, and the computation for forward substituion will follow the below reduced code: \newline
\pseudoCode \newline
  \textbf {for} k=1:n-1 \newline
  \hspace*{1cm} $y_{k} = b_{k}$ \newline
   $y_{n}$ = $\dfrac{b_{n} - \sum_{j=1}^{n-1} C_{n,j}y_{j}}{1}$ \newline

 \textbf {Cost for forward substitution} = 2n $\approx  O(n)$ \newline

Next, for the backward substituion for Ux=y, the computation will follow the below reduced code: \newline
\pseudoCode \newline
  $x_{n}$ = $\dfrac{y_{n}}{C_{n,n}}$ \newline
  \textbf {for} k = n-1:1 \newline
   \hspace*{0.5cm} $x_{k}$ = $\dfrac{y_{k} - C_{k,n}*x_{n}}{C_{k,k}}$ \newline

\textbf {Cost for backward Substituion} = 3n $\approx O(n)$

Thus, total cost for solving Ax=b for the matrix form of B = $O(n) + O(n) + O(n) \approx O(n)$

\end{document} 






























