\documentclass{article}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{longtable}
\newcommand{\bx}{{\bf x}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bv}{{\bf v}}
\newcommand{\by}{{\bf y}}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\newcommand\algo{\vspace{.10in}\textbf{Algorithm: }}
\newcommand\correctness{\vspace{.10in}\textbf{Correctness: }}
\newcommand\runtime{\vspace{.10in}\textbf{Running time: }}
\newcommand\pseudoCode{\vspace{.10in}\textbf{PseudoCode: }}
\newcommand*{\perm}[2]{{}^{#1}\!P_{#2}}
\newcommand*{\comb}[2]{{}^{#1}\!C_{#2}}
%\pagestyle{fancyplain}
%\lhead{\textbf{\NAME\ (\UID)}}
%\chead{\textbf{Hw\HWNUM}}
%\rhead{CS 6350, \today}
\title{CS6210 - Homework/Assignment-4}
\author{Arnab Das(u1014840)}
\usepackage[utf8]{inputenc}
\begin{document}
  \pagenumbering{gobble}
  \maketitle
  \newpage
  \pagenumbering{arabic}
  \newcommand\NAME{ARNAB DAS}
  \newcommand\UID{uxxxxxxx}
  \newcommand\HWNUM{4}

  %%---- Chapter-6 --------------%%
  \question{1}{Chapter-6, question-4} 
  \part{a} The techniques discussed in this chapter are for polynomial data fitting and not exponential data fitting, hence cannot be applied directly to $u(t)$ . \newline
  
  \part{b} Given approximation of the form
  \[ u(t) = \gamma_1 \exp(\gamma_2t)\]
  and provided data points as $(t_1,z_1),(t_2,z_2), \dots , (t_m,z_m)$, where $z_i > 0, i=1,2,\dots,m$ and $m>0$. \newline
  Considering instead the following approximation: \newline
  \[ v(t) = lnu(t) = (ln\gamma_1) + \gamma_2t \]
  such that the data points become $(t_1,b_1),(t_2,b_2), \dots , (t_m,b_m)$ where $b_i = lnz_i$ and 
  \[v(t) = x_1 + x_2t\]
  such that $x_1 = ln\gamma_1$ and $x_2 = \gamma_2$ and $v(t) = lnu(t)$ \newline
  Then for solving $A^TAx = A^Tb$, we define the following matrices: \newline
  $A$ = $\begin{bmatrix}
	  1 & t_1 \\
	  1 & t_2 \\
	  \dots & \dots \\
	  1 & t_m \\
\end{bmatrix}$ leading to 
 $B = A^TA$ =  $\begin{bmatrix}
	 \sum_{i=1}^m 1 = m & \sum_{i=1}^m t_i \\
	 \sum_{i=1}^m t_i & \sum_{i=1}^m {t_{i}}^2 \\ 	 
 \end{bmatrix}$ and
 $b$ = $\begin{bmatrix}
	 t_1 \\
	 t_2 \\
	 \dots \\
	 t_m \\
\end{bmatrix}$ leading to 
$A^Tb$ = $\begin{bmatrix}
	\sum_{i=1}^m b_i \\
	\sum_{i=1}^m t_ib_i \\
\end{bmatrix}$
Thus solving
\[ B = A^TAx = A^Tb\]
we get the following two equations: \newline
\begin{equation}
	x_1 + x_2 = 1
\end{equation}
and 
\begin{equation}
	3x_1 + 5x_2 = 4.9
\end{equation}
Solving (i) and (ii) , we get: $x_1 = ln\gamma_1 = 0.5$ and $x_2 = \gamma_2 = 0.95$ \newline
Thus, we have , 
\[v(t) = lnu(t) = ln(0.5) + 0.95t\]
\[=> u(t) = 0.5 \times exp(0.95t)\]
(Answer). \newline

\question{2}{Chapter-6, question-5}
\part{a} For tall and skinny matrices, A, of the system of equations , $Ax=b$, the number of rows is much larger than  the number of columns. Let the number of rows be m and the number of columns n, then in such cases generally $m >> n$. These systems are overdetermined and b is generally not in the range space of A. Thus applying LU does not gives a unique solution to Ax=b. Instead, a better way to approach such problems is to minimize the residual $||b-Ax||$, such that within the tolerance of the residual we have the best solution for x. When transforming to the normal equation, $A^TAx = A^Tb$, here one can use LU decomposition since we have $n\times n$ matrix, however cholesky decomposition is a better choice here since the matrix $A^TA$ is symmetric positive definite. In case of the QR decomposition, which has an upper triangular part in R, however the Q matrix allows us to extract the upper-triangular system of equation whose solution leads to a solution of the least square problem. Here, the orthonormal behaviour of Q is used to transform into equivalent set of equations such that the \textbf {norms are not affected}. Finally, SVD is used more in cases where A is rank deficient or nearly rank deficient, in which cases LU cannot be used. Thus, in all the cases LU directly doesn't fits the scenario for application except that LU requires to be slightly modified so that for every column it zeroes out, the vector of its remaining elements is orthonormal to the previous columns. With this introduction of orthonormality, it can be used in QR decomposition. \newline

\part{b} In the normal equation: the way condition number came to be $K(B) = K^2(A)$. This was because of the folliwing derivation: \newline
In normal equation we were solving: 
\[A^TAx = A^Tb\] and
\[ (A^TA)^{-1}A^T = VS^{-2}V^T V\sum^{T}U^T = V(S^{-1} 0)U^T\]
where A = $U\sum V^T$ , V and U are orthogonal matrices and $\sum$ is a diagonal matrix with singular values of A along the diagonal, and hence $A^TA = V\sum U^T U\sum V^T$ = $V \sum^2 V^T$ \newline
Thus $||(A^TA)^{-1}A^T||$ = $||(S^{-1} 0)||$ and $||((A^TA)^{-1}A^T)^{-1}|| = ||V(S 0)U^T||$\newline
Thus the condition number becomes: \newline
 \[K(B=A^TA) = \dfrac{\lambda_1}{\lambda_n} = \dfrac{\sigma_{1}^2}{\sigma_{n}^2} = K^2(A)\]

Now, for QR, we write $x = (A^TA)^{-1}A^T b $, and we have represented A=QR by a QR decomposition where Q is orthonormal and of same dimension as A. The following translation results in the final form: \newline
	\[x = (A^TA)^{-1}A^T b = (R^TQ^TQR)^{-1}R^TQ^Tb = (R^TR)^{-1}R^TQ^Tb) = R^{-1}Q^Tb\]
	Hence, we get the following relation,
	\[ x = R^{-1}Q^Tb\]
	Notice, that multiplication by orthogonal matrices do not affect the norms, thus,
	\[||R^{-1}Q^T|| = ||R^{-1}|| = ||A^{-1}||\]
	and also, 
	\[||(Q^T)^{-1}R|| = ||R|| = ||A||\]
	Thus the condition number in this case comes to be $||A||||A^{-1}|| = K(A)$. \newline
	The main saving comes due to usage of the orthonormal decomposition for the transformations, since the transformations using q only transofrms them in space without affecting the norm values, while for the previous normal equation, the singular values were getting multiplied during creation $A^TA$, introducing change in norm values during the transformations. \newline

\question{3}{Chapter-8, question-8}
 \part{a} Given a rank deficient matrix, we analyse here its effect on the Gra,-Schmidt process. Consider the span of three vectors $v_1, v_2 and v_3$, such that $v_3$ is linearly dependent on $v_1$ and $v_2$. We can write $v_3$ as a linear combination of $v_1$ and $v_2$, as: \newline
 \[\bf v_3 = a\bf v_1 + b\bf v_2\]
 Let $\bf u_i$ denote the orthonormal unit vectors we generate along the Gram-Scmidt process. Then, for the first vector, $\bf v_1$, we have 
 \[\bf u_1 = \dfrac{\bf v_1}{|\bf v_1|}\]
 And the orthogonal transformation of $\bf v_2$ that is orthogonal to $\bf v_1$ is
 \[ \bf y_2 = \bf v_2 - (\bf v_2.\bf u_1)\bf u_1\]

 Then, the next orthonormal vector, $\bf u_2$ will be along $\bf y_2$, such that $\bf u_2 = \dfrac{\bf y_2}{|\bf y_2|}$. \newline
 Now the orthogonal transformation of $\bf v_3$, say $\bf y_3, $ that is orthogonal to the span($\bf u_1, \bf u_2$), will be, \newline
 \[\bf y_3 = \bf v_3 - \bigg ( (\bf v_3 . \bf u_1)\bf u_1 + (\bf v_3 . \bf u_2)\bf u_2  \bigg )\]
 \[\bf y_3 = \bf v_3 - \bigg ( ((a\bf v_1 + b\bf v_2).\bf u_1).\bf u_1 + ((a\bf v_1 + b\bf v_2).\bf u_2)\bf u_2 \bigg )\]
 \[\bf y_3 = \bf v_3 - \bigg ( a\bf v_1 + (b\bf v_2.\bf u_1)\bf u_1 + (a\bf v_1.\bf u_2)\bf u_2 + (b\bf v_2.\bf u_2)\bf u_2 \bigg )\]
 Here: \newline
 $(b\bf v_2.\bf u_1)\bf u_1 = b(\bf v_2 - \bf y_2)$ \newline
 $(a\bf v_1.\bf u_2)\bf u_2 = 0$ \newline
 $(b\bf v_2.\bf u_2)\bf u_2 = b\bf y_2$ \newline

 Replacing them and the value of $\bf v_3$ in the above equation we get: \newline
 \[ \bf y_3 = a\bf v_1 + b\bf v_2 - ( a\bf v_1 + b\bf v_2 - b\bf y_2 + 0 + b\bf y_2 ) = 0\]
 Thus $\bf y_3$, comes out to be zero, when we encounter the linearly dependent vector. (Answer). \newline

 \part{b} The classical pseudoCode of Gram-Scmidt is as below: \newline
 \textbf {Classical-GS} \newline
 \hspace*{0.5cm} $\bf for$ k=1:n \newline
 \hspace*{1.0cm} w = $a_k$ \newline
 \hspace*{1.0cm} $\bf for$ j=1:k-1 \newline
 \hspace*{1.5cm} $r_{jk} = q_{j}^Tw$ \newline
 \hspace*{1.0cm} end \newline
 \hspace*{1.0cm} $\bf for$ j=1:k-1 \newline
 \hspace*{1.5cm} w = w - $r_{jk}q_j$ \newline
 \hspace*{1.0cm} end \newline
 \hspace*{1.0cm} $r_{kk} = ||w||$ \newline
 \hspace*{1.0cm} $q_{k} = w/r_{kk}$ \newline
 \hspace*{0.5cm} end \newline

 \textbf {Modified-GS} \newline
 \hspace*{0.5cm} $\bf for$ k=1:n \newline
 \hspace*{1.0cm} w = $a_k$ \newline
 \hspace*{1.0cm} $\bf for$ j=1:k-1 \newline
 \hspace*{1.5cm} $r_{jk} = q_{j}^Tw$ \newline
 \hspace*{1.5cm} $w = w - r_{jk}q_{j}$ \newline
 \hspace*{1.0cm} end \newline
 \hspace*{1.0cm} $r_{kk} = ||w||$ \newline
 \hspace*{1.0cm} $q_{k} = w/r_{kk}$ \newline
 \hspace*{0.5cm} end \newline

 Suppose at the k'th iteration the orthonormal q's already calculated are $Q_{k-1} = [q_1, q_2, \dots, q_{k-1}]$. In the \textbf {classical} case, we first calculate the projections, so suppose the calculated values of the projections in the k'th iteration are \newline
 \[[r_{ik}, r_{2k}, \dots, r_{k-1,k}]\]
 where, $r_{jk} = q_{j}^Tw$,  where w in the k'th iteration is initialised to $a_{k}$, the vector whose orthogonal transformation is done in the k'th step. Then, these projections along the corresponding orthonormal directions are subtracted from w, in the second inner loop, resulting in: \newline
 \begin{equation}
  w = w - r_{1k}\bf q_1 - r_{2k}\bf q_2 - \dots - r_{k-1,k}\bf q_{k-1}
 \end{equation}

 In the modified version, instead of precomputing all the $r_{jk}$'s at a time, we compute $r_{jk}$ and substract from w, thus always $orthogonalizing$ against the currently computed version. Suppose, we are in the k'th iteration, and j=1 results in the following computation
 \[ w_{1} = w - r_{1k}q_1 = w - (q_{1}^Tw)q_1 \]
 Then for j=2 in the k'th iteration: \newline
 \[ w_{2} = w_1 - r_{2k}q_2 =  w - (q_{1}^Tw)q_1 -   q_{2}^T(w - (q_{1}^Tw)q_1)q_2   \]
 \[ w_{2} = w_1 - r_{2k}q_2 =  w - (q_{1}^Tw)q_1 -   (q_{2}^T.w)q_2 - 0   \]
 \[ w_{2} = w_1 - r_{2k}q_2 =  w - r_{1k}\bf q_1 -   r_{2k}\bf q_2 - 0   \]

 And similarly for increasing j's(j limits to k) it holds true since $q_i and q_j$ are orthonromal for $i\neq j$. Thus , in exact arithmatic the modified and classical version are numerically the same. (Proved). \newline

 \part{c}

 \question{4}{Chapter-7, Question-9}
 For the iterative scheme, we have $x_{k+1} = x_{k} + \alpha_{k}p_{k}$, where, $p_{k}$ is the search direction and $\alpha_{k}$ is the step size. This includes the basic statioanry methods as well of the form, $x_{k+1} = x_{k} + M^{-1}r_{k}$.\newline
 \part{a}
 Now, consider the given iterative scheme: \newline
 \[ x_{k+1} = x_{k} + \alpha(b - Ax_{k}) \]
 Since, $r_{k} = b - Ax_{k}$ is the residual at the k'th step, and in gradient descent , the search direction is in the reverse direction of the residual, Hence, $p_{k} = r_{k}$. Hence, for a fixed $\alpha$ we get: 
 \[M^{-1} = \alpha I\]
 \[M = (\alpha I)^{-1}\]
 (answer). \newline
 The iteration matrix(T) is defined as \newline
 \[T = I - M^{-1}A = I - \alpha I A = I - \alpha A\]
 (answer). \newline

  \part{b}
  \bf {(i)}Given that A is symmetric positive definite and its eigen values follows the inequality: \newline
  \begin{equation}
	  \lambda_1 > \lambda_2 > \dots > \lambda_n > 0
  \end{equation}
  Scaling equation(4) by $\alpha$, we get the following
	\[  \alpha\lambda_1 > \alpha\lambda_2 > \dots > \alpha\lambda_n > 0 \]
	\[  -\alpha\lambda_1 < -\alpha\lambda_2 < \dots < -\alpha\lambda_n < 0 \]
 \begin{equation}
	 1 -\alpha\lambda_1 < 1 -\alpha\lambda_2 < \dots < 1 -\alpha\lambda_n < 1 
 \end{equation}

 Since, $T = I - \alpha A$, then for an eigen value $\lambda_i$ of A, their will be a corresponding eigen value of T as $(1 - \alpha \lambda_i)$. Thus, equation-5 gives the eigen values of T and the order of their relative magnitudes. \newline

 The theorem for Statioanry method converegence says, that if the spectral radius of the iteration matrix is less than 1, then the system converges. The spectral radius of a matrix is defined as : \newline
 $\rho (B)$ = max{$|\lambda_i|: \lambda_i$ are the eigen values of B} \newline
 Equation(5) shows that the spectral radius of T will be $|1 - \alpha \lambda_n|$ and for convergence it should be less than 1. Hence, \newline
 \[-1 < 1 - \alpha \lambda_n < 1\]
 \begin{equation}
  0 < \alpha < \dfrac{2}{\lambda_n}
 \end{equation}
 (condition on $\alpha$ for convergence).


  \bf {(ii)} The condition number of a symmetric positive definite matrix,A, is given by: \newline
  \[K(A) = \dfrac{\lambda_{max}}{\lambda_{min}} = \dfrac{\lambda_1}{\lambda_n}\]
  \[ \lambda_n = \dfrac{\lambda_1}{K(A)}\]
  and substituting this in equation(6) \newline
  \begin{equation}
	  0 < \alpha < \dfrac{2K(A)}{\lambda_1}
  \end{equation}
  (Condition on $\alpha$ in terms of the condition number) \newline

  \bf {(iii)} For the statioanry method, at the i'th ieration and for iteration matrix T, the following error relation holds: \newline
  \[ ||e_i|| \leq ||T||^i e_0\]

  Also, for the steepest descent at the i'th iteration we have the following relation,
  \[ ||e_i|| \leq \bigg ( \dfrac{k-1}{k+1} \bigg )^i e_0\]
  where k is the condition number. Since steepest descent is the one that converegs at the best speed for the class of gradient descent, we can equate these two relations to get: \newline
  \[ ||T|| = \dfrac{k-1}{k+1}\]
  \[ (1-\alpha \lambda_n) = \dfrac{k-1}{k+1}\]
  \[ \alpha \lambda_n = \dfrac{2}{k+1} = \dfrac{2\lambda_n}{\lambda_1 + \lambda_n}\]
  \[ \alpha = \dfrac{2}{\lambda_1 + \lambda_n}\]
  (Best value for $\alpha$ for maximizing speed) . \newline

  \part{c} "If A is strictly diagonally dominant and $\alpha = 1$, then the iterative scheme converegs to the solution for any initial guess." \newline
  Since, $\alpha = 1$, our M is basically the identity matrix. Note that, for any matrix equation, Ax=b, we can scale each row of A and corresponding element in B by the corresponding  diagonal element of that row such that the system of equation remains unalterted. By performing such an operation we can ensure that all the diagonal elements are 1, but the diagonal dominance of the original matrix still remains. Since , our M is the identity matrix, and the diagonal matrix of A is also the identity matrix, this means in our problem we chose the splitting of M to be the diagonal matrix, which means we are performing the Jacobi iteration. Thus it just remains to prove that Jacobi iterations converge if the matrix is diagonally dominant.  \newline
  \textbf {Proof:} \newline
  By defintition of diagonal dominance, we have:
  \[ |a_{ii} > \sum_{j\neq i}|a_{ij}|\]
  \[ \sum_{j\neq i} \dfrac{a_{ij}}{a_{ii}} < 1\]
  For Convergence, we are required to show that $||M^{-1}N|| < 1$. Here we pick the infinity norm: \newline
  \[ ||G||_{\infty} = ||M^{-1}N|| = ||D^{-1}(L+U)||_{\infty} = max_{1\leq i \leq m} \sum_{j\neq i} \dfrac{|a_{ij}|}{a_{ii}} < 1 \]

  Hence Jacobi converges, and by consequence of that, if a Matrix is stricly diagonally dominant and $\alpha=1$, then the iterative scheme converges to the solution for any initial guess. (Proved). \newline

  \question{5}{Chapter-7, question-12}
   Given a linear system , Ax=b, where A is symmetric. Suppose M-N is a splitting of A, where M is symmetric positive definite and N is symmetric. \newline
   Since, M is symmetric Positive Definite, hence we can write the condition number of A as, \newline
  \[K(A) = ||M||||M^{-1}||\dfrac{\lambda_{max}(M)}{\lambda_{min}(M)}\]
  or, from here we get: \newline
  \begin{equation}
	  ||M^{-1}|| = \dfrac{\lambda_{max}(M)}{\lambda_{min}(M) \times ||M||}
  \end{equation}

  Now, for an iterative scheme with a splitting of A=M-N, we have: \newline
  \[ x_{k+1} = M^{-1}Nx_k + M^{-1}b \]
  This converegs if the norm of $M^{-1}N$ is less than 1.\newline
  \[ ||M^{-1}N|| \leq ||M^{-1}|| ||N|| =  \dfrac{\lambda_{max}(M)}{\lambda_{min}(M) \times ||M||} ||N|| \]
  Now the norm of a symmetric positive definite matrix is the magnitude of its highest eigen-vector. Hence, $||M|| = \lambda_{max}$, and hence they cancel each other. Since, it is given that $\lambda_{min}(M) > \rho(N)$, and for a symmetric matrix N, $\rho(N) \leq ||N||$. Thus, we get the following: \newline
  \[||M^{-1}N|| \leq \dfrac{||N||}{\lambda_{min}(M)} \leq \dfrac{\rho(N)}{\lambda_{min}(M)} < 1 \]

  Hence, the system converegs if $\rho(N) < \lambda_{min}(M)$ (Proved). \newline



  \question{6}{GMRES ----}

  \question{7}{Chapter-8, question-7}
  \part{a}







\end{document}
