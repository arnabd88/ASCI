\documentclass{article}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{longtable}
\newcommand{\bx}{{\bf x}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bv}{{\bf v}}
\newcommand{\by}{{\bf y}}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headheight}{13.6pt}
\newcommand\question[2]{\vspace{.25in}\hrule\textbf{#1: #2}\hrule\vspace{.10in}}
\renewcommand\part[1]{\vspace{.10in}\textbf{(#1)}}
\newcommand\algo{\vspace{.10in}\textbf{Algorithm: }}
\newcommand\correctness{\vspace{.10in}\textbf{Correctness: }}
\newcommand\runtime{\vspace{.10in}\textbf{Running time: }}
\newcommand\pseudoCode{\vspace{.10in}\textbf{PseudoCode: }}
\newcommand*{\perm}[2]{{}^{#1}\!P_{#2}}
\newcommand*{\comb}[2]{{}^{#1}\!C_{#2}}
%\pagestyle{fancyplain}
%\lhead{\textbf{\NAME\ (\UID)}}
%\chead{\textbf{Hw\HWNUM}}
%\rhead{CS 6350, \today}
\title{CS6210 - Homework/Assignment-4}
\author{Arnab Das(u1014840)}
\usepackage[utf8]{inputenc}
\begin{document}
  \pagenumbering{gobble}
  \maketitle
  \newpage
  \pagenumbering{arabic}
  \newcommand\NAME{ARNAB DAS}
  \newcommand\UID{uxxxxxxx}
  \newcommand\HWNUM{4}

  %%---- Chapter-6 --------------%%
  \question{1}{Chapter-6, question-4} 
  \part{a} The techniques discussed in this chapter are for polynomial data fitting and not exponential data fitting, hence cannot be applied directly to $u(t)$ . \newline
  
  \part{b} Given approximation of the form
  \[ u(t) = \gamma_1 \exp(\gamma_2t)\]
  and provided data points as $(t_1,z_1),(t_2,z_2), \dots , (t_m,z_m)$, where $z_i > 0, i=1,2,\dots,m$ and $m>0$. \newline
  Considering instead the following approximation: \newline
  \[ v(t) = lnu(t) = (ln\gamma_1) + \gamma_2t \]
  such that the data points become $(t_1,b_1),(t_2,b_2), \dots , (t_m,b_m)$ where $b_i = lnz_i$ and 
  \[v(t) = x_1 + x_2t\]
  such that $x_1 = ln\gamma_1$ and $x_2 = \gamma_2$ and $v(t) = lnu(t)$ \newline
  Then for solving $A^TAx = A^Tb$, we define the following matrices: \newline
  $A$ = $\begin{bmatrix}
	  1 & t_1 \\
	  1 & t_2 \\
	  \dots & \dots \\
	  1 & t_m \\
\end{bmatrix}$ leading to 
 $B = A^TA$ =  $\begin{bmatrix}
	 \sum_{i=1}^m 1 = m & \sum_{i=1}^m t_i \\
	 \sum_{i=1}^m t_i & \sum_{i=1}^m {t_{i}}^2 \\ 	 
 \end{bmatrix}$ and
 $b$ = $\begin{bmatrix}
	 t_1 \\
	 t_2 \\
	 \dots \\
	 t_m \\
\end{bmatrix}$ leading to 
$A^Tb$ = $\begin{bmatrix}
	\sum_{i=1}^m b_i \\
	\sum_{i=1}^m t_ib_i \\
\end{bmatrix}$
Thus solving
\[ B = A^TAx = A^Tb\]
we get the following two equations: \newline
\begin{equation}
	x_1 + x_2 = 1
\end{equation}
and 
\begin{equation}
	3x_1 + 5x_2 = 4.9
\end{equation}
Solving (i) and (ii) , we get: $x_1 = ln\gamma_1 = 0.5$ and $x_2 = \gamma_2 = 0.95$ \newline
Thus, we have , 
\[v(t) = lnu(t) = ln(0.5) + 0.95t\]
\[=> u(t) = 0.5 \times exp(0.95t)\]
(Answer). \newline

\question{2}{Chapter-6, question-5}
\part{a} For tall and skinny matrices, A, of the system of equations , $Ax=b$, the number of rows is much larger than  the number of columns. Let the number of rows be m and the number of columns n, then in such cases generally $m >> n$. These systems are overdetermined and b is generally not in the range space of A. Thus applying LU does not gives a unique solution to Ax=b. Instead, a better way to approach such problems is to minimize the residual $||b-Ax||$, such that within the tolerance of the residual we have the best solution for x. When transforming to the normal equation, $A^TAx = A^Tb$, here one can use LU decomposition since we have $n\times n$ matrix, however cholesky decomposition is a better choice here since the matrix $A^TA$ is symmetric positive definite. In case of the QR decomposition, which has an upper triangular part in R, however the Q matrix allows us to extract the upper-triangular system of equation whose solution leads to a solution of the least square problem. Here, the orthonormal behaviour of Q is used to transform into equivalent set of equations such that the \textbf {norms are not affected}. Finally, SVD is used more in cases where A is rank deficient or nearly rank deficient, in which cases LU cannot be used. Thus, in all the cases LU directly doesn't fits the scenario for application except that LU requires to be slightly modified so that for every column it zeroes out, the vector of its remaining elements is orthonormal to the previous columns. With this introduction of orthonormality, it can be used in QR decomposition. \newline

\part{b} In the normal equation: the way condition number came to be $K(B) = K^2(A)$. This was because of the folliwing derivation: \newline
In normal equation we were solving: 
\[A^TAx = A^Tb\] and
\[ (A^TA)^{-1}A^T = VS^{-2}V^T V\sum^{T}U^T = V(S^{-1} 0)U^T\]
where A = $U\sum V^T$ , V and U are orthogonal matrices and $\sum$ is a diagonal matrix with singular values of A along the diagonal, and hence $A^TA = V\sum U^T U\sum V^T$ = $V \sum^2 V^T$ \newline
Thus $||(A^TA)^{-1}A^T||$ = $||(S^{-1} 0)||$ and $||((A^TA)^{-1}A^T)^{-1}|| = ||V(S 0)U^T||$\newline
Thus the condition number becomes: \newline
 \[K(B=A^TA) = \dfrac{\lambda_1}{\lambda_n} = \dfrac{\sigma_{1}^2}{\sigma_{n}^2} = K^2(A)\]

Now, for QR, we write $x = (A^TA)^{-1}A^T b $, and we have represented A=QR by a QR decomposition where Q is orthonormal and of same dimension as A. The following translation results in the final form: \newline
	\[x = (A^TA)^{-1}A^T b = (R^TQ^TQR)^{-1}R^TQ^Tb = (R^TR)^{-1}R^TQ^Tb) = R^{-1}Q^Tb\]
	Hence, we get the following relation,
	\[ x = R^{-1}Q^Tb\]
	Notice, that multiplication by orthogonal matrices do not affect the norms, thus,
	\[||R^{-1}Q^T|| = ||R^{-1}|| = ||A^{-1}||\]
	and also, 
	\[||(Q^T)^{-1}R|| = ||R|| = ||A||\]
	Thus the condition number in this case comes to be $||A||||A^{-1}|| = K(A)$. \newline
	The main saving comes due to usage of the orthonormal decomposition for the transformations, since the transformations using q only transofrms them in space without affecting the norm values, while for the previous normal equation, the singular values were getting multiplied during creation $A^TA$, introducing change in norm values during the transformations. \newline

\question{3}{Chapter-8, question-8}
 \part{a} Given a rank deficient matrix, we analyse here its effect on the Gra,-Schmidt process. Consider the span of three vectors $v_1, v_2 and v_3$, such that $v_3$ is linearly dependent on $v_1$ and $v_2$. We can write $v_3$ as a linear combination of $v_1$ and $v_2$, as: \newline
 \[\bf v_3 = a\bf v_1 + b\bf v_2\]
 Let $\bf u_i$ denote the orthonormal unit vectors we generate along the Gram-Scmidt process. Then, for the first vector, $\bf v_1$, we have 
 \[\bf u_1 = \dfrac{\bf v_1}{|\bf v_1|}\]
 And the orthogonal transformation of $\bf v_2$ that is orthogonal to $\bf v_1$ is
 \[ \bf y_2 = \bf v_2 - (\bf v_2.\bf u_1)\bf u_1\]

 Then, the next orthonormal vector, $\bf u_2$ will be along $\bf y_2$, such that $\bf u_2 = \dfrac{\bf y_2}{|\bf y_2|}$. \newline
 Now the orthogonal transformation of $\bf v_3$, say $\bf y_3, $ that is orthogonal to the span($\bf u_1, \bf u_2$), will be, \newline
 \[\bf y_3 = \bf v_3 - \bigg ( (\bf v_3 . \bf u_1)\bf u_1 + (\bf v_3 . \bf u_2)\bf u_2  \bigg )\]
 \[\bf y_3 = \bf v_3 - \bigg ( ((a\bf v_1 + b\bf v_2).\bf u_1).\bf u_1 + ((a\bf v_1 + b\bf v_2).\bf u_2)\bf u_2 \bigg )\]
 \[\bf y_3 = \bf v_3 - \bigg ( a\bf v_1 + (b\bf v_2.\bf u_1)\bf u_1 + (a\bf v_1.\bf u_2)\bf u_2 + (b\bf v_2.\bf u_2)\bf u_2 \bigg )\]
 Here: \newline
 $(b\bf v_2.\bf u_1)\bf u_1 = b(\bf v_2 - \bf y_2)$ \newline
 $(a\bf v_1.\bf u_2)\bf u_2 = 0$ \newline
 $(b\bf v_2.\bf u_2)\bf u_2 = b\bf y_2$ \newline

 Replacing them and the value of $\bf v_3$ in the above equation we get: \newline
 \[ \bf y_3 = a\bf v_1 + b\bf v_2 - ( a\bf v_1 + b\bf v_2 - b\bf y_2 + 0 + b\bf y_2 ) = 0\]
 Thus $\bf y_3$, comes out to be zero, when we encounter the linearly dependent vector. (Answer). \newline

 \part{b} The classical pseudoCode of Gram-Scmidt is as below: \newline
 \textbf {Classical-GS} \newline
 \hspace*{0.5cm} $\bf for$ k=1:n \newline
 \hspace*{1.0cm} w = $a_k$ \newline
 \hspace*{1.0cm} $\bf for$ j=1:k-1 \newline
 \hspace*{1.5cm} $r_{jk} = q_{j}^Tw$ \newline
 \hspace*{1.0cm} end \newline
 \hspace*{1.0cm} $\bf for$ j=1:k-1 \newline
 \hspace*{1.5cm} w = w - $r_{jk}q_j$ \newline
 \hspace*{1.0cm} end \newline
 \hspace*{1.0cm} $r_{kk} = ||w||$ \newline
 \hspace*{1.0cm} $q_{k} = w/r_{kk}$ \newline
 \hspace*{0.5cm} end \newline

 \textbf {Modified-GS} \newline
 \hspace*{0.5cm} $\bf for$ k=1:n \newline
 \hspace*{1.0cm} w = $a_k$ \newline
 \hspace*{1.0cm} $\bf for$ j=1:k-1 \newline
 \hspace*{1.5cm} $r_{jk} = q_{j}^Tw$ \newline
 \hspace*{1.5cm} $w = w - r_{jk}q_{j}$ \newline
 \hspace*{1.0cm} end \newline
 \hspace*{1.0cm} $r_{kk} = ||w||$ \newline
 \hspace*{1.0cm} $q_{k} = w/r_{kk}$ \newline
 \hspace*{0.5cm} end \newline

 Suppose at the k'th iteration the orthonormal q's already calculated are $Q_{k-1} = [q_1, q_2, \dots, q_{k-1}]$. In the \textbf {classical} case, we first calculate the projections, so suppose the calculated values of the projections in the k'th iteration are \newline
 \[[r_{ik}, r_{2k}, \dots, r_{k-1,k}]\]
 where, $r_{jk} = q_{j}^Tw$,  where w in the k'th iteration is initialised to $a_{k}$, the vector whose orthogonal transformation is done in the k'th step. Then, these projections along the corresponding orthonormal directions are subtracted from w, in the second inner loop, resulting in: \newline
 \begin{equation}
  w = w - r_{1k}\bf q_1 - r_{2k}\bf q_2 - \dots - r_{k-1,k}\bf q_{k-1}
 \end{equation}

 In the modified version, instead of precomputing all the $r_{jk}$'s at a time, we compute $r_{jk}$ and substract from w, thus always $orthogonalizing$ against the currently computed version. Suppose, we are in the k'th iteration, and j=1 results in the following computation
 \[ w_{1} = w - r_{1k}q_1 = w - (q_{1}^Tw)q_1 \]
 Then for j=2 in the k'th iteration: \newline
 \[ w_{2} = w_1 - r_{2k}q_2 =  w - (q_{1}^Tw)q_1 -   q_{2}^T(w - (q_{1}^Tw)q_1)q_2   \]
 \[ w_{2} = w_1 - r_{2k}q_2 =  w - (q_{1}^Tw)q_1 -   (q_{2}^T.w)q_2 - 0   \]
 \[ w_{2} = w_1 - r_{2k}q_2 =  w - r_{1k}\bf q_1 -   r_{2k}\bf q_2 - 0   \]

 And similarly for increasing j's(j limits to k) it holds true since $q_i and q_j$ are orthonromal for $i\neq j$. Thus , in exact arithmatic the modified and classical version are numerically the same. (Proved). \newline

 \part{c}

 \question{4}{Chapter-7, Question-9}
 For the iterative scheme, we have $x_{k+1} = x_{k} + \alpha_{k}p_{k}$, where, $p_{k}$ is the search direction and $\alpha_{k}$ is the step size. This includes the basic statioanry methods as well of the form, $x_{k+1} = x_{k} + M^{-1}r_{k}$.\newline
 \part{a}
 Now, consider the given iterative scheme: \newline
 \[ x_{k+1} = x_{k} + \alpha(b - Ax_{k}) \]
 Since, $r_{k} = b - Ax_{k}$ is the residual at the k'th step, and in gradient descent , the search direction is in the reverse direction of the residual, Hence, $p_{k} = r_{k}$. Hence, for a fixed $\alpha$ we get: 
 \[M^{-1} = \alpha I\]
 \[M = (\alpha I)^{-1}\]
 (answer). \newline
 The iteration matrix(T) is defined as \newline
 \[T = I - M^{-1}A = I - \alpha I A = I - \alpha A\]
 (answer). \newline

  \part{b}
  \bf {(i)}Given that A is symmetric positive definite and its eigen values follows the inequality: \newline
  \begin{equation}
	  \lambda_1 > \lambda_2 > \dots > \lambda_n > 0
  \end{equation}
  Scaling equation(4) by $\alpha$, we get the following
	\[  \alpha\lambda_1 > \alpha\lambda_2 > \dots > \alpha\lambda_n > 0 \]
	\[  -\alpha\lambda_1 < -\alpha\lambda_2 < \dots < -\alpha\lambda_n < 0 \]
 \begin{equation}
	 1 -\alpha\lambda_1 < 1 -\alpha\lambda_2 < \dots < 1 -\alpha\lambda_n < 1 
 \end{equation}

 Since, $T = I - \alpha A$, then for an eigen value $\lambda_i$ of A, their will be a corresponding eigen value of T as $(1 - \alpha \lambda_i)$. Thus, equation-5 gives the eigen values of T and the order of their relative magnitudes. \newline

 The theorem for Statioanry method converegence says, that if the spectral radius of the iteration matrix is less than 1, then the system converges. The spectral radius of a matrix is defined as : \newline
 $\rho (B)$ = max{$|\lambda_i|: \lambda_i$ are the eigen values of B} \newline
 Equation(5) shows that the spectral radius of T will be $|1 - \alpha \lambda_n|$ and for convergence it should be less than 1. Hence, \newline
 \[-1 < 1 - \alpha \lambda_n < 1\]
 \begin{equation}
  0 < \alpha < \dfrac{2}{\lambda_n}
 \end{equation}
 (condition on $\alpha$ for convergence).


  \bf {(ii)} The condition number of a symmetric positive definite matrix,A, is given by: \newline
  \[K(A) = \dfrac{\lambda_{max}}{\lambda_{min}} = \dfrac{\lambda_1}{\lambda_n}\]
  \[ \lambda_n = \dfrac{\lambda_1}{K(A)}\]
  and substituting this in equation(6) \newline
  \begin{equation}
	  0 < \alpha < \dfrac{2K(A)}{\lambda_1}
  \end{equation}
  (Condition on $\alpha$ in terms of the condition number) \newline

  \bf {(iii)} For the statioanry method, at the i'th ieration and for iteration matrix T, the following error relation holds: \newline
  \[ ||e_i|| \leq ||T||^i e_0\]

  Also, for the steepest descent at the i'th iteration we have the following relation,
  \[ ||e_i|| \leq \bigg ( \dfrac{k-1}{k+1} \bigg )^i e_0\]
  where k is the condition number. Since steepest descent is the one that converegs at the best speed for the class of gradient descent, we can equate these two relations to get: \newline
  \[ ||T|| = \dfrac{k-1}{k+1}\]
  \[ (1-\alpha \lambda_n) = \dfrac{k-1}{k+1}\]
  \[ \alpha \lambda_n = \dfrac{2}{k+1} = \dfrac{2\lambda_n}{\lambda_1 + \lambda_n}\]
  \[ \alpha = \dfrac{2}{\lambda_1 + \lambda_n}\]
  (Best value for $\alpha$ for maximizing speed) . \newline

  \part{c} "If A is strictly diagonally dominant and $\alpha = 1$, then the iterative scheme converegs to the solution for any initial guess." \newline
  Since, $\alpha = 1$, our M is basically the identity matrix. Note that, for any matrix equation, Ax=b, we can scale each row of A and corresponding element in B by the corresponding  diagonal element of that row such that the system of equation remains unalterted. By performing such an operation we can ensure that all the diagonal elements are 1, but the diagonal dominance of the original matrix still remains. Since , our M is the identity matrix, and the diagonal matrix of A is also the identity matrix, this means in our problem we chose the splitting of M to be the diagonal matrix, which means we are performing the Jacobi iteration. Thus it just remains to prove that Jacobi iterations converge if the matrix is diagonally dominant.  \newline
  \textbf {Proof:} \newline
  By defintition of diagonal dominance, we have:
  \[ |a_{ii} > \sum_{j\neq i}|a_{ij}|\]
  \[ \sum_{j\neq i} \dfrac{a_{ij}}{a_{ii}} < 1\]
  For Convergence, we are required to show that $||M^{-1}N|| < 1$. Here we pick the infinity norm: \newline
  \[ ||G||_{\infty} = ||M^{-1}N|| = ||D^{-1}(L+U)||_{\infty} = max_{1\leq i \leq m} \sum_{j\neq i} \dfrac{|a_{ij}|}{a_{ii}} < 1 \]

  Hence Jacobi converges, and by consequence of that, if a Matrix is stricly diagonally dominant and $\alpha=1$, then the iterative scheme converges to the solution for any initial guess. (Proved). \newline

  \question{5}{Chapter-7, question-12}
   Given a linear system , Ax=b, where A is symmetric. Suppose M-N is a splitting of A, where M is symmetric positive definite and N is symmetric. \newline
   Since, M is symmetric Positive Definite, hence we can write the condition number of A as, \newline
  \[K(A) = ||M||||M^{-1}||\dfrac{\lambda_{max}(M)}{\lambda_{min}(M)}\]
  or, from here we get: \newline
  \begin{equation}
	  ||M^{-1}|| = \dfrac{\lambda_{max}(M)}{\lambda_{min}(M) \times ||M||}
  \end{equation}

  Now, for an iterative scheme with a splitting of A=M-N, we have: \newline
  \[ x_{k+1} = M^{-1}Nx_k + M^{-1}b \]
  This converegs if the norm of $M^{-1}N$ is less than 1.\newline
  \[ ||M^{-1}N|| \leq ||M^{-1}|| ||N|| =  \dfrac{\lambda_{max}(M)}{\lambda_{min}(M) \times ||M||} ||N|| \]
  Now the norm of a symmetric positive definite matrix is the magnitude of its highest eigen-vector. Hence, $||M|| = \lambda_{max}$, and hence they cancel each other. Since, it is given that $\lambda_{min}(M) > \rho(N)$, and for a symmetric matrix N, $\rho(N) \leq ||N||$. Thus, we get the following: \newline
  \[||M^{-1}N|| \leq \dfrac{||N||}{\lambda_{min}(M)} \leq \dfrac{\rho(N)}{\lambda_{min}(M)} < 1 \]

  Hence, the system converegs if $\rho(N) < \lambda_{min}(M)$ (Proved). \newline



  \question{6}{Chapter-7, question-20}
  \part{a} We are required to solve the the following:
  \[min_z ||\rho \bf e_1 - \bf H_{k+1,k}\bf z||\]
  where $H_{k+1,k}$ is a Hessenberg Matrix. We use here Givens rotation to perform the QR decomposition of the Hessenberg matrix. The Givens rotation matrix is an identity matrix except the specific co-ordinates whose rotations are to be performed. It is written as, \newline
  $G(i,j,\theta) = \begin{bmatrix} 
	  1 & \dots & 0 & \dots & 0 & \dots & 0 \\
	  \dots & \dots & c & \dots & -s & \dots & 0 \\
	  \dots & \dots & \dots & \dots & \dots & \dots & \dots \\
	  \dots & \dots & s & \dots & c & \dots & 0 \\
	  \dots & \dots & \dots & \dots & \dots & \dots & \dots \\
	  0 & \dots & 0 & \dots & 0 & \dots & 1 \\
  \end{bmatrix}$ where, $c = cos\theta,$ and $s = sin\theta $\newline 
   that is, all the diagonal elements are 1 except for a given(i,j), where we place the c and s values such that the $2 \times 2$ matrix is formed by them is orthogonal. It affects only the rows i and j and zeros out the (i,j) entry of the matrix. Consider we want to zero out the (i,j) entry such that (i,i) entry is a and (i,j) entry is b. The the following leads to the the required result \newline
   $\begin{bmatrix}
	   c & -s \\
	   s & c \\
   \end{bmatrix}$ $\begin{bmatrix}
	   a \\
	   b \\
   \end{bmatrix}$ \newline
   if we define $r = \sqrt[2]{a^2 + b^2}$ and $c = \dfrac{a}{r}$ and $s = \dfrac{-b}{r}$. This results in $sa + cb=0$, exactly what we want, and this matrix $\begin{bmatrix}
	   c & -s \\
	   s & c \\
   \end{bmatrix}$ is also orthonormal.  Hence, the entire G matrix is orthonormal. Thus, we repeatedly apply G to the original matrix to zero out the elements below its diagonal for a Hessenberg matrix that would result in an upper-traiangular matrix hence completing the QR decomposition.\newline
   Then, in the first iteration we need to remove (2,1), so:
   \[H_1 = G_{2,1}H\]
   Then, we need to remove (3,2) 
   \[ H_2 = G_{3,2}H_1 = G_{3,2}G_{2,1}H\]
   and in this way continuing to k iterations, we get: 
   \[R = H_k = G_{k+1,k}\dots G_{2,1}H\]
   This R is now an upper triangular matrix. Since the G's are orthonormal, we multiply both sides by their transpose, to get 
   \[H = G_{2,1}^T G_{3,2}^T \dots G_{k+1,k}^T R\]
   \[H = QR\]
   This completes our QR decomposition using Givens rotation, such that 
   \[Q = G_{2,1}^T G_{3,2}^T \dots G_{k+1,k}^T\]

   Note that,Q here is now $(k+1)\times(k+1)$ matrix and R is upper triangular $(k+1)\times k$, with the last row of R being all zeros. \newline
   In the program, we initialize Q as an identity matrix of size $(k+1)\times(k+1)$. We loop over $i=2->k+1$, and since it is Hessenberg, we essentially need to zero out (i,i-1). So, we set $j=i-1$. If the matrix in the current iterate is H, then we get the following :
   \[ a = H(i-1,j)\]
   \[ b = H(i,j)\]
   \[ r = \sqrt[2]{a^2 + b^2}\]
   \[ c = \dfrac{a}{r}, s=-\dfrac{b}{r}\]
   Next, we declare a matrix, $G_{i}$ as an idenity mattrix of size (k+1). We update this matrix with the values of c and s at the specific points.
   \[ G_{i}(i,i) = c\]
   \[ G_{i}(j,j) = c\]
   \[ G_{i}(i,j) = s\]
   \[ G_{i}(j,i) = -s\]
   Then evaluate , $H = G_{i}H$, this results in H(i,j) becoming 0. We update Q as $Q*G_{i}^\prime$. At the end of the loop, R=H has the upper triangular matrix and Q is the orthonormal matrix of QR decomposition.\newline
   So, now we have ,
   \[H_{k+1,k} = Q_{k+1,k+1}R_{k+1,k}\]
   We can now write,
   \[||\rho e_1 - Q_{k+1}R_{k+1,k}z||\]
   \[||Q_{k+1,k+1}^T \rho e_1 - R_{k+1,k}z||\]
   \[||Q_{k+1,k+1}^T \rho e_1 - {{R_{k,k}}\choose{0}}z||\]
   we can solve for the k,k components of , and the remaining will be the residual. Hence, solve for 
   \[||Q_{k,k}^T \rho e_1 - R_{k,k}z||\]
   Hence, we solve from here:
   \[z = R_{k,k}^{-1} Q_{k,k}||\rho||e_1\]

   Call the program as $minimizer(\rho,H,sz) $, where H is the Hessenberg matrix and sx is the value of k for $H_{k+1,k}$. Program file is minimizer.m. It calls internally givens1.m, where the givens rotation is implemented. \newline


  \question{7}{Chapter-8, question-7}
  \part{a} Given a column stochastic matrix,P, of size $n \times n$ whose all entries are non-negative and each column sum to 1. 
   \[A(\alpha) = \alpha P + (1-\alpha)E \]
   For this matrix,A, the entry at its i'th row and j'th column will be
   \[a_{ij} = \alpha p_{ij} + \dfrac{1-\alpha}{n}\]
   Then the sum of the elements of its j'th column will be:
   \[ \sum_{i=1}^n a_{ij} = \sum_{i=1}^n (\alpha p_{ij} + \dfrac{1-\alpha}{n}) \]
   \[ \sum_{i=1}^n a_{ij} = \alpha \sum_{i=1}^n p_{ij} + \dfrac{1-\alpha}{n}\sum_{i=1}^n 1 \]
   Since P is also stochastic, hence $\sum_{i=1}^n p_{ij} = 1$, thus we get: \newline
   \[ \sum_{i=1}^n a_{ij} = \alpha + \dfrac{1-\alpha}{n}\times n = 1 \]
   Thus, the sum of elements of a column in A is also 1. Hence, A is also stochastic. (Proved). \newline

   \part{b} A column stochastic matrix, or a Markov matrix,'A', has an eigen value of 1 and all others are less than 1. So, $\lambda = 1$, is the dominant eigen value. If A is column stochastic, then $A^T$ is row-stochastic, thus the sum of the elements of each row of $A^T$ is 1. So: \newline
  $A^T$ $\begin{bmatrix}
	  1 \\
	  1 \\
	  \dots \\
	  1 \\
  \end{bmatrix}$ = $\begin{bmatrix}
	  1 \\
	  1 \\
	  \dots \\
	  1 \\
  \end{bmatrix}$ \newline
  Hence, $A^T$ has an eigen value of 1. Since the determinants of $det(A - \lambda_nI)$ and $det(A^T - \lambda_nI)$ are the same, hence A and $A^T$ has the same eigen value. \textbf {Thus, A has an eigen value of 1}. \newline
  Now, suppose A has an eigen vector , say v, whose eigen-value $\lambda$ is greater than 1, that is, $|\lambda| > 1$. This implies that $A^nv = \lambda^nv$ has exponentially growing length for large $n -> \infty$. This further implies that there is a large coefficient $[A^n]_{i,j}$ which is larger than 1, since A is non-negative. Since, matrix multiplication of two stochastic matrices results in stochastic matrix, hence as A is stochastic, so $A^n$ is also stochastic which indicates that all its entries has to be $\leq 1$, providing a contradiction. So, all eigen values other than 1, has to be less than 1. Hence, the dominant eigen value is 1(Proved). \newline
  Let the dominant eigen-vector be \bf v, and its corresponding eigen value is 1. \newline
  Hence,
  \[ A(\alpha)v = v = \alpha Pv + (1-\alpha)Ev\]
  \[(I - \alpha P)v = (1 - \alpha)Ev\]

  Now, Ev = $\begin{bmatrix}
	  	1/n & 1/n & \dots & 1/n \\
	  	1/n & 1/n & \dots & 1/n \\
		\dots & \dots & \dots & \dots \\
	  	1/n & 1/n & \dots & 1/n \\
  \end{bmatrix}$ v = $\dfrac{1}{n}\begin{bmatrix}
	  			   1 \\
	  			   1 \\
	  			   \dots \\
	  			   1 \\
	  			 \end{bmatrix}$, as the eigen-vectors are also stochastic vectors. Substituing them in the original equation, we get: \newline

  $(I - \alpha P)v = (1 - \alpha)\dfrac{1}{n} \begin{bmatrix}
	  			   1 \\
	  			   1 \\
	  			   \dots \\
	  			   1 \\
	  			 \end{bmatrix}$ \newline
  \bf v = $\dfrac{1-\alpha}{n} (I - \alpha P)^{-1}\begin{bmatrix}
	  			   1 \\
	  			   1 \\
	  			   \dots \\
	  			   1 \\
  \end{bmatrix}$ \textbf {This is the dominant eigen-vector of $A(\alpha)$}. \newline

  \part{c} Suppose the second dominant eigen value of A is $\lambda_2$ and its corresponding eigen-vector is x. Then: 
  \begin{equation}
	  \alpha Px + (1-\alpha)Ex = \lambda_2 x
  \end{equation}
  We will come back to equation(9) after proving the following lemma. \newline
  \textbf {lemma-1:} If $x_i$ is the eigen vector of A with eigen value $\lambda_i$ and $y_j$ is the eigen vector of $A^T$ with eigen value $\lambda_j$, then if $\lambda_i \neq \lambda_j$, then $x_{i}^Ty_j = 0$ \newline
  \textbf {proof:} We can write the following :
  \[ y_{j}^TA = \lambda_{j}y_{j}^T \]
  \[ Ax_{i} = \lambda_{i}x_{i}\]
  Transposing and then Multiplying the first by $x_{i}^T$ and the second by $y_j$, we get
  \[ x_{i}^TA^Ty_j = \lambda_ix_{i}^Ty_j\]
  \[ x_{i}^TA^Ty_j = \lambda_jx_{i}^Ty_j\]
  Subtracting the above leads to the fact that is $\lambda_i \neq \lambda_j$, then:
  \[x_{i}^Ty_{j} = 0\]
  (Proved) \newline

  Coming back to our original equation(9), notice that E's columns are vectors $e^T=\dfrac{1}{n}[1,1, \dots, 1]$. This all 1 vector is an eigen-vector of $A^T$ and corresponds to eigen-value of 1. Thus, applying the lemma we just proved, we then get $Ex_2=0$. Now the equation(9) becomes, 
  \[\alpha Px_2 = \lambda_2 x_2 \]
  \[Px_2 = \dfrac{\lambda_2}{\alpha }x_2\]
  Since, P is also stochastic and the $x_2$ is also an eigen vector of P and 1 is the dominant eigen vector, hence: \newline
  \[\dfrac{\lambda_2}{\alpha} \leq 1\]
  \[\lambda_2 \leq \alpha\]
  Thus, the second largest wigenvalue of $A(\alpha)$ is upper bounded by $\alpha$. \newline

  \part{d}
  \textbf {(i)} $A(\alpha)$ contains to essential components , one of them $P(\alpha)$ is large and sparse, while E is large and sense. Since P is very sparse, it can have a compresses row or compressed column representation and we perform matvec operations on this compresses structure over only the non-zero entries. Additionally, we do not require any storage for the last row entry of the matrix A. This is because, Since A is stochastic, the column sums of A are 1, hence the last element of the matvec  will be all the previous elements of the result subtracted from the column sum of the vector. So, at every step of the sparse matrix vector product, we keep accumulating the sum, until we reach the last row, where-in we completely abandon the computation and subtract this sum from the column sum of the vector, and that gives us the last element. \newline
The next saving we do for A, is by utilizing the structure of E. Note that although E is large and dense, it is just a rank-1 matrix. Hence , multiplying a vector, $v = [v_1, v_2, \dots, v_n]$ to E, results in the following: \newline
$\begin{bmatrix}
	1/n & 1/n & \dots & 1/n \\
	1/n & 1/n & \dots & 1/n \\
	\dots & \dots & \dots & \dots \\
	1/n & 1/n & \dots & 1/n \\
\end{bmatrix}$ $\begin{bmatrix}
	v_1 \\
	v_2 \\
	\dots \\
	v_n \\
\end{bmatrix}$ = $\begin{bmatrix} 
	\dfrac{v_1 + v_2 + \dots + v_n}{n} \\
	\dfrac{v_1 + v_2 + \dots + v_n}{n} \\
	\dots \\
	\dfrac{v_1 + v_2 + \dots + v_n}{n} \\
\end{bmatrix}$ = $\sum_{i=1}^n v_i.\dfrac{1}{n} \begin{bmatrix}
	1 \\
	1 \\
	\dots \\
	1 \\
\end{bmatrix}$ \newline
Thus every multiplication with E, results in a scalar value that scales the all 1 column space, and this value is essentially the sum of the elements of the vector. Thus, instead of computing the dense matrix vector multiplication of $O(n^2)$, we only require summing the vector elements which is $O(n)$. \newline

 \textbf {(ii)} If the initial guess $v_0$ satisfies $||v_0||_1 = 1$, and supposing $v_0$ is non-negative , that means $v_0$ is a column stochastic vector. Since A is a stochastic matrix, if we show that the matrix vector product of a stochastic matrix and a stochastic vector results in a stochastic vector, then we can conclude that the result of each iteration of the power-method will result in a stochastic vector whose l1-norm will be 1 and hence normalization will not be required. \newline
 \textbf {Proof:} Let A be an $n \times n$ stochastic matrix and v be a $n \times 1$ stochastic vector. \newline
 Then, $\sum_{i=1}^n a_{ij} = 1$, for all j=1,2,... and $\sum_{j=1}^n v_{j} = 1$, then \newline
 Av = $\begin{bmatrix}
	 \sum_{j=1}^n a_{1j}.v_j \\
	 \sum_{j=1}^n a_{2j}.v_j \\
	 \dots \\
	 \sum_{j=1}^n a_{2j}.v_j \\
 \end{bmatrix}$
 Then, $||Av||$ = \[\sum_{i=1}^n \sum_{j=1}^n a_{ij}.v_j = \sum_{j=1}^n (\sum_{i=1}^n a_{ij}).v_j = \sum_{j=1}^n v_j = 1\]

 Thus, the result of the multipication of a stochastic vector by a stochastic matrix results in a stochastic vector whose l1-norm is 1. (Proved). \newline

 \question{8}{chapter-8,question-10}
 Given the least squares problem
 \[  min_x ||b - Ax||_2\]
 where A is ill conditioned. Considering the regularization approach that replaces the the normal equations by the modified, better conditioned system.
 \begin{equation}
 (A^TA + \gamma I)x_{\gamma} = A^Tb
 \end{equation}
 \part{a} 
 \[K_2(A^TA + \gamma I) = \sqrt[2]{\dfrac{\lambda_{max}^2 + \gamma}{\lambda_{min}^2 + \gamma}} \]
 \[ = \sqrt[2]{\dfrac{\lambda_{max}^2(1 + \dfrac{\gamma}{\lambda_{max}^2})}{\lambda_{min}^2(1 + \dfrac{\gamma}{\lambda_{min}^2})}}\]
 Now, since, $\lambda_{max} > \lambda_{min}$, hence $\dfrac{1}{\lambda_{max}} < \dfrac{1}{\lambda_{min}}$, and using this in the previous equation , we get
 \[K_2(A^TA + \gamma I) = \sqrt[2]{\dfrac{\lambda_{max}^2(1 + \dfrac{\gamma}{\lambda_{max}^2})}{\lambda_{min}^2(1 + \dfrac{\gamma}{\lambda_{min}^2})}} \leq \sqrt[2]{\dfrac{\lambda_{max}^2}{\lambda_{min}^2}} =  \dfrac{\lambda_{max}}{\lambda_{min}} = K_2^2(A)\]
 (Proved). \newline

 \part{b} 
 We will reformulate the equations for $x_{y}$ as a least square problem. Equation-10 can be written as $Cx_\gamma = D$, and to translate it to a least square problem, we would like to minimize the residual, that is $||D - Cx_\gamma||$. We ca summarize the following data for relations between C,D,A and b. \newline
 We can write , $A = U\Sigma V^T$ and correspondingly $A^T = V\Sigma U^T$, where U,V are orthogonal matrices and $\Sigma$ is a diagonal matrix of singular values. Then $A^TA = V \Sigma^2 V^T$ and $AA^T = U\Sigma^2 U^T$. We have defined $C = (A^TA + \gamma I)$, which can be further broken into: \newline
 \[C = A^TA + \gamma I = V\Sigma^2V^T + \gamma I = V(\Sigma^2 + \gamma I)V^T\]

 and $D = A^Tb = (V\Sigma U^T)b$ \newline

 Now, relacing C and D into the minimization form, we get
 \[ ||D - Cx_\gamma|| = ||A^Tb - (A^TA + \gamma I)x_\gamma|| = || (V\Sigma U^T)b - V(\Sigma^2 + \gamma I)V^T x_\gamma|| \]
 Since V is orthogonal, we can safely multiply by $V^T$ without changing the norms. Hence,
 \[|| V^T(V\Sigma U^T)b - V^TV(\Sigma^2 + \gamma I)V^T x_\gamma|| = || (\Sigma U^T)b - (\Sigma^2 + \gamma I)V^T x_\gamma||\]
 This can be written in the form:
 \[||z - \Sigma^\prime y ||\]
 where, z = $U^Tb$, y = $V^Tx_\gamma$, and $\Sigma^\prime = \Sigma^2 + \gamma I$.If the rank of the matrix is $r < n$, set $y_i = 0$, for $i=r+1,r+2,\dots,n $ and set the other $y_i$ for $0 \leq i \leq r$, as:
 \[ y_i = \dfrac{z_i}{((\sigma_i)^\prime)} = \dfrac{\sigma_i . {u_{i}}^Tb}{(\sigma_i)^2 + \gamma}\]
 Then compute $\bf {x_\gamma=Vy} $. $x_\gamma$ in terms of the columns $u_i$ of U and $v_i$ of V can be written as : 
 \[x_\gamma = \sum_{i=1}^r \dfrac{\sigma_i {u_i}^Tb}{{\sigma_i}^2 + \gamma} = V\Sigma^{\prime\prime}U^T\]
 where, 
 \[\Sigma^{\prime\prime} = 0, \sigma_i = 0\]
 \[\Sigma^{\prime\prime} = \dfrac{\sigma_i}{{\sigma_i}^2 + \gamma}, \sigma_i \neq 0\]

\part{c}

 Then, 
 \[||x_\gamma|| = ||V\Sigma^{\prime\prime}U^T|| = ||\Sigma^{\prime\prime}||\]
 Since V and U are orthogonal

 Now:
 \begin{equation}
 \dfrac{\sigma_i}{{\sigma_i}^2 + \gamma} = \dfrac{1}{\sigma_i + \dfrac{\gamma}{\sigma_i}} \leq \dfrac{1}{\sigma_i}
\end{equation}
 Since , for the original equation, the final x is written as $x=V\Sigma^*U^T$, where the $\Sigma^*$ is defined as 
 \[\Sigma^* = 0 , \sigma_i = 0\]
 \[\Sigma^* = \dfrac{1}{\sigma_i} , \sigma_i \neq 0\]

 Similarly,
 \[||x|| = ||\Sigma^*||\]
 Then , due to equation(11), we have $||\Sigma^{\prime\prime}|| \leq ||\Sigma^*||$. Hence, we get:
 \begin{equation}
	 ||x_\gamma|| \leq ||x||
 \end{equation}
 (Proved)

 \part{d}
	 For the original equation we have 
	 \[A^TAx = A^Tb\]
	 \[Bx = A^Tb\]
	 \[x = B^{-1}A^Tb\]
	where $B = A^TA$
	 For the regularized equation , we have
	 \[(A^TA + \gamma I)x_\gamma = A^Tb\]
	 \[ B^{-1}(B + \gamma I)x_\gamma = (B^{-1})A^Tb\]
	 \[ x_\gamma + B^{-1}\gamma x_\gamma = x\]
	 \begin{equation}
	  x - x_\gamma = B^{-1}\gamma x_\gamma
	\end{equation}
	Taking the norms we get:
	\[||x - x_\gamma|| \leq ||B^{-1}||||\gamma||||x_\gamma|| \leq ||B^{-1}||||\gamma||||x||\]
	Now, $||B^{-1}|| = ||(A^TA)^{-1}|| =  \dfrac{1}{\sigma_{min}^2}$, hence:
	\[\dfrac{||x - x_\gamma||}{||x||} \leq \dfrac{\gamma}{\sigma_{min}^2}\]
	(answer). \newline

	To guarantee that the relative error is bounded by $\epsilon$, we can write: \newline
	\[ \dfrac{\gamma}{\sigma_{min}^2} \leq \epsilon \]
	\[ \gamma \leq \epsilon \times \sigma_{min}^2 \]


\part{e}
   The matlab code for this part can be found as \textbf {Prob8.m}. Comparing the result for $\gamma = 0$ with that of example-8.8, the svd computation differs from matlab's backslash(using QR) by 0.0002 for $||x||$ since we get $||x||=0.9473$ while book's answer is 0.9471. Note here, that for non-zero small $\gamma$ values in order of $10^{-6}, 10^{-12}$ we get the similar result as reported in example-8.8, which points towards improved accuracy with slightly perturbed $\gamma$. The residual norm comes to be the same for $\gamma=0$ and that of the book's example. \newline


 \part{f} 
   For ill-conditioned problems, where the condition number is very large, indicating a large ratio of the max and min singular values, it is difficult to tune the truncation of the svd. Instead, using the regularization approach, as we derived in (d) of this question, there is a nice way to bound the relative error, using the $\gamma$ as a knob for tuning and hence provides better control on the relatove error. \newline






\end{document}
